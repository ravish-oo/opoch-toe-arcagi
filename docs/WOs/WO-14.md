WO-14 you asked for—frozen to v1.5 + Patch v1.6, pure CS, no heuristics, receipts-first. It defines the **feature extractor** and the **size predictor** (H1–H7), plus an optional **constant-color map**. No stubs, no ambiguity.

# WO-14 — Aggregate Mapping (T9): Features & Size Predictor

## Purpose

1. Compute a **frozen, integer-only feature vector** from inputs (and, where needed, from training outputs for period stats).
2. Evaluate the **frozen hypothesis class H1–H7** on **trainings only** to pick a **single size function**; then predict the test output size ((R_{\text{out}},C_{\text{out}})).
3. (Optional) Provide a **constant-color mapping** table (e.g., argmax-color → uniform output), strictly if it fits all trainings.

---

## Inputs & dependencies

* Depends on **WO-02** (KMP minimal **proper** periods p≥2, phase=(0,0)).
* Depends on **WO-05** (per-color 4-CC counts and area stats).
* Uses the **original grids** (no frames): raw (X_i, Y_i) for each training, and (X^*) for the test size only.

**Color universe:** ( \mathcal C = {0} \cup \mathrm{colors}(X^*) \cup \bigcup_i \mathrm{colors}(X_i) \cup \bigcup_i \mathrm{colors}(Y_i) ), ordered ascending integers (A.1). Counts use this order.

---

## Interfaces (pure; no I/O; frozen signatures)

```python
from typing import Dict, List, Tuple, Optional, TypedDict

class FeatureVector(TypedDict):
    H: int                 # input height
    W: int                 # input width
    counts: Dict[int,int]  # per-color counts over X
    cc: Dict[int, Dict[str,int]]  # per-color 4-CC stats over X (see below)
    periods: Dict[str, Optional[int]]  # row_min, col_min, lcm_r, lcm_c, gcd_r, gcd_c (proper periods only; else None)

def agg_features(X: List[List[int]], C_order: List[int]) -> FeatureVector:
    """
    Compute frozen features on input X (and period stats from X only).
    Uses WO-05 for 4-CC stats; WO-02 for proper minimal periods (p>=2) row/col.
    All fields are integers or None. No floats, no heuristics.
    """
```

> **Note:** Periods here are on **X** (input). If your earlier spec used output periods for H5, you can add an **extra** “out_periods” field later (add-only). For WO-04a we only need a consistent period source; pick input or output and **freeze** it. To stay self-contained, we freeze to **input periods** here.

```python
class SizeFit(TypedDict):
    winner: Dict[str, object]                 # {"family":"Hk","params":{...}}
    attempts: List[Dict[str, object]]         # ordered attempts with ok_train_ids, fit_all
    verified_train_ids: List[int]

def agg_size_fit(train_pairs: List[Dict[str, List[List[int]]]],
                 features: List[FeatureVector]) -> SizeFit | None:
    """
    Evaluate H1..H7 in frozen order using *trainings only*.
    Return a size fit object if some hypothesis fits all trainings; else None.
    """
```

```python
def predict_size(H_star: int, W_star: int, fit: SizeFit) -> Tuple[int,int]:
    """
    Apply the winning hypothesis to the test input size (H*, W*) to get (R_out, C_out).
    Raises ValueError if 'fit' is None.
    """
```

```python
# Optional (constant-color map):
class ColorMap(TypedDict):
    mapping: Dict[str, int]   # e.g., {"ARGMAX_COLOR": 8} meaning “uniform 8”
    winner: str               # family tag, e.g., "CONST_ARGMAX"
    attempts: List[Dict[str,object]]

def agg_color_map(train_pairs: List[Dict[str, List[List[int]]]], 
                  features: List[FeatureVector], 
                  C_order: List[int]) -> Optional[ColorMap]:
    """
    Learn a *constant-color* mapping if (and only if) it reproduces all training outputs exactly:
    Y_i must be uniform with that color. Otherwise return None.
    """
```

---

## Feature vector (frozen schema)

Given (X) (H×W), colors (C_order):

* `H, W`: integers.
* `counts[c]`: number of pixels with value `c` in X for **each** c in `C_order`.
* `cc[c]`: per-color 4-CC stats over **X** (use WO-05):

  * `n`: number of components,
  * `area_min`, `area_max`, `area_sum` (integers; if `n=0`, set min/max to 0 and keep sum=0).
* `periods` over **X** (use WO-02 minimal **proper** KMP periods p≥2):

  * `row_min` := the **minimal common proper period** across rows if all rows share a proper period; else None.
  * `col_min` := analogous for columns.
  * `lcm_r` := LCM of all **proper** per-row minimal periods found (ignoring rows with None); if none, None.
  * `lcm_c`, `gcd_r`, `gcd_c` analogously (GCD ignores None; if no periods, None).

> All period fields are **integers or None**, no 1 allowed (p≥2 only). Phase=(0,0) fixed.

---

## Size hypotheses (H1–H7) — **frozen definition & order**

Evaluate these **in order**; the first **family+params** that fits **all** trainings wins.

Let training (i) have input size ((H_i,W_i)) and observed output size ((R_i,C_i)).

1. **H1: Multiplicative**
   (R'_i = a H_i,\ C'_i = c W_i), with (a,c \in {1,\dots,8}).

2. **H2: Additive**
   (R'_i = H_i + b,\ C'_i = W_i + d), with (b,d \in {0,\dots,16}).

3. **H3: Mixed affine**
   (R'_i = a H_i + b,\ C'_i = c W_i + d), with above bounds.

4. **H4: Constant**
   (R'_i = R_0,\ C'_i = C_0) (same constants for all trainings).
   Search (R_0, C_0 \in {1,\dots,30}) (ARC bounds).

5. **H5: Period (lcm/gcd) from **input periods** (frozen)

   * Candidate rows: if `lcm_r` is not None: (R'_i = k_r \cdot \mathrm{lcm}_r) for (k_r \in {1..8}).
     Else, skip row LCM.
   * Candidate cols: if `lcm_c` is not None: (C'_i = k_c \cdot \mathrm{lcm}_c) for (k_c \in {1..8}).
     Else, skip col LCM.
   * Combine rows/cols candidates **Cartesian** with missing side using identity (`R'_i = H_i` if no row period, `C'_i = W_i` if no col period).
   * A parameter tuple `(k_r, k_c)` **fits** if for **every** training, the predicted size equals observed ((R_i,C_i)).

6. **H6: Floor stride (integer downscale)**
   (R'_i = \lfloor H_i / k_r \rfloor,\ C'_i = \lfloor W_i / k_c \rfloor,\ k_r,k_c \in {2..5}).
   (WO-04a does **size only**; content checks happen later in Unanimity normalization.)

7. **H7: Ceil stride**
   (R'_i = \lceil H_i / k_r \rceil,\ C'_i = \lceil W_i / k_c \rceil,\ k_r,k_c \in {2..5}).

**Fit criterion (strict):** For a given family and param tuple, require ((R'_i,C'_i) = (R_i,C_i)) for **all** trainings (i). No partial fits.

**Tie rule (frozen):**

* First, **smallest area** on test input: compute ((R^*,C^*)) from ((H^*,W^*)) and compare (R^* \cdot C^*).
* Then, **family id** lex (“H1” < “H2” < …).
* Then, **parameter tuple lex** (e.g., `(a,c)`, `(b,d)`, `(a,b,c,d)`, `(k_r,k_c)`).

If **no family** fits: return `None` (WO-04a will raise `SIZE_UNDETERMINED` with the attempts trail).

---

## Optional constant-color mapping (independent of size)

Only emit this if it is **exact on all trainings**:

* Families to try (frozen order):

  1. `CONST_ARGMAX`: pick uniform color `argmax_c h_i(c)` **on the training’s input** `X_i` (ties → **lex-min color**).
  2. `CONST_MAJORITY_OUT`: pick uniform color equal to the **majority color in Y_i** (ties → lex-min).
* **Fit:** for each training, (Y_i) must be a **uniform** grid of the selected color.
* Winner chosen by family order (both produce a single color, no size).
* Receipts contain `mapping: {"ARGMAX_COLOR": u}` or similar; if no family fits all trainings, return `None`.

---

## Receipts (first-class; additive; stable)

### Features

Section: `"features"` (one per grid X you compute on, or aggregate per training id).

* Minimum: `features_hash` per training (BLAKE3 over the **canonical JSON** of FeatureVector with stable key order; include `colors_order` alongside if you want).
* If you choose to store full features in the receipt (optional): keep types strict (ints/None only).

### Size fit (per task)

Section: `"size_fit"` (returned by `agg_size_fit`):

```json
{
  "attempts": [
    {"family":"H1","params":{"a":2,"c":3},"ok_train_ids":[0,1,2],"fit_all":true},
    {"family":"H2","params":{"b":1,"d":0},"ok_train_ids":[0,2],"fit_all":false},
    ...
  ],
  "winner": {"family":"H1","params":{"a":2,"c":3}},
  "verified_train_ids":[0,1,2],
  "section_hash":"…"
}
```

If **no fit** (`None`): WO-04a will embed the same `attempts` and a `first_counterexample` in its `working_canvas` receipt.

### Constant color (optional)

Section: `"color_map"` if present:

```json
{
  "attempts":[
    {"family":"CONST_ARGMAX","ok_train_ids":[0,1,2],"fit_all":true},
    {"family":"CONST_MAJORITY_OUT","ok_train_ids":[0,2],"fit_all":false}
  ],
  "winner":{"family":"CONST_ARGMAX","params":{}},
  "mapping":{"ARGMAX_COLOR":8},
  "section_hash":"…"
}
```

**Determinism:** Always compute BLAKE3 over the stable JSON. Double-run must be identical.

---

## Invariants & failure modes

* **Trainings-only selection** (no test leakage).
* **Integer-only**: no floats, no heuristics, no search beyond finite parameter grids.
* **Determinism**: frozen order of families and parameter enumeration; tie rules as above.
* **Failure:** if no size hypothesis fits, `agg_size_fit` returns `None`. WO-04a converts this into `SIZE_UNDETERMINED` with receipts.

---

## Exact parameter enumeration order (to avoid drift)

* H1: iterate `a` from 1..8 outer; for each `a`, iterate `c` 1..8 inner.
* H2: iterate `b` 0..16 outer; for each `b`, `d` 0..16 inner.
* H3: nest `(a,c)` outer (a 1..8, c 1..8), then `(b,d)` inner (b 0..16, d 0..16).
* H4: `R0` 1..30 outer; for each, `C0` 1..30 inner.
* H5: if `lcm_r` is None skip row; if `lcm_c` is None skip col; otherwise `k_r` 1..8 outer, `k_c` 1..8 inner. If only one side present, iterate its k 1..8.
* H6/H7: `k_r` 2..5 outer, `k_c` 2..5 inner.

> **Tie rule** uses test area; enumeration order doesn’t affect final choice but **does affect `attempts`**. Freeze it as above so receipts are stable.

---

## Edge cases (fully specified)

* **Single training**: allowed.
* **Zero trainings**: return `None`.
* **Periods None**: H5 skips the missing side(s).
* **Argmax ties** (optional color map): pick **lex-min color** (frozen).
* **Counts sum check**: assert (\sum_c h(c) = H \cdot W).

---

## What’s explicitly out of scope here

* Any content normalization or downsample/replicate checks (those belong to **Unanimity** in T2, later).
* Any palette/σ logic (belongs to Witness).
* Any model-based inference: this is pure arithmetic over H,W and frozen feature integers.

---

## Developer checklist (Implementer)

* Implement `agg_features` exactly: counts, WO-05 CC stats, WO-02 proper periods → lcm/gcd.
* Implement `agg_size_fit` with **frozen family order** and **bounded** param grids; fill the `attempts` trail for every candidate evaluated.
* Implement `predict_size(H*,W*,fit)` by applying the winning family/params to test size; raise if `fit` is None.
* (Optional) Implement `agg_color_map` only if you can verify **uniform** Y_i on all trainings; else return `None`.
* Emit receipts sections `features`, `size_fit` (and `color_map` if used) with BLAKE3 hashes.

---

## Reviewer quick-verification (real ARC; 1–2 lines)

* On tasks with varying training sizes, `size_fit` must pick a single winner with an ordered `attempts` trail; applying `predict_size` to ((H^*,W^*)) yields ((R_{\text{out}},C_{\text{out}})).
* On an “argmax-uniform” task, `color_map` (if enabled) must reconstruct all training outputs exactly and select ties lex-min.

---

**Spec grounding:**

* Features: v1.5 §4.1 evidence (counts, components/invariants, periods).
* Size: Patch v1.6 “Working canvas prediction” H1–H7, frozen order and tie rules; trainings-only; test size applied after selection.
* No LCM: WO-14 doesn’t lift/scale any content—only picks a size function with receipts.
